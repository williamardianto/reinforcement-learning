{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pygame\n",
    "# pip install pygame\n",
    "\n",
    "# install ple\n",
    "# git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "# cd PyGame-Learning-Environment/\n",
    "# pip install -e .\n",
    "\n",
    "# install gym_ple\n",
    "# pip install gym_ple\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Utils import Environment, SumTree\n",
    "import os\n",
    " \n",
    "import matplotlib\n",
    "from matplotlib.pyplot import imshow, show\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "        self.available = False\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def add(self, state, action, reward, next_state):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        state = torch.FloatTensor([state])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "        experience = self.Transition(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        self.tree.add(max_priority, experience)  # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            memory_b.append(data)\n",
    "            \n",
    "            sample = self.Transition(*(zip(*memory_b)))\n",
    "        \n",
    "        return b_idx, sample, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "            \n",
    "    def is_ready(self):\n",
    "        if self.available:\n",
    "            return True\n",
    "        else:\n",
    "            prio_min = np.min(self.tree.tree[-self.tree.capacity:])\n",
    "            self.available = prio_min != 0\n",
    "            return self.available\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, Box(512, 288, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x109f1b8d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.init()\n",
    "env.step(1)\n",
    "imshow(env.get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DDDQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value_fc = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "#         self.fc2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        value_fc = self.value_fc(x)\n",
    "        advantage_fc = self.advantage_fc(x)\n",
    "        \n",
    "        # Agregating layer\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        output = value_fc + (advantage_fc - advantage_fc.mean(1,keepdim=True))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dddqn = DDDQN()\n",
    "test = torch.randn(5,4,84,84)\n",
    "dddqn(test).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.dqn = DDDQN().to(device)\n",
    "        self.target = DDDQN().to(device)\n",
    "        self.target.load_state_dict(self.dqn.state_dict())\n",
    "        self.target.eval()\n",
    "        self.target_update_interval = 1000\n",
    "#         self.optimizer = optim.RMSprop(self.dqn.parameters())\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.0001)\n",
    "        \n",
    "        #play params\n",
    "        self.play_interval = 1000\n",
    "        self.play_repeat = 1\n",
    "        self.best_steps_done = 0\n",
    "        self.best_score = -1000\n",
    "        \n",
    "        self.seed = 111\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_decay = 100000\n",
    "        self.epsilon_stop = 0.01\n",
    "        self.batch_size = 32\n",
    "#         self.max_step = 1000\n",
    "        self.global_steps = 0\n",
    "        \n",
    "        self.memory = Memory(10)\n",
    "\n",
    "        self.frame_skipping = 4\n",
    "        self.state_buffer_size = 4\n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        \n",
    "    def save_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        dirpath = os.path.dirname(filename)\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.mkdir(dirpath)\n",
    "\n",
    "        checkpoint = {\n",
    "            'dqn': self.dqn.state_dict(),\n",
    "            'target': self.target.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'global_steps': self.global_steps,\n",
    "            'best_score': self.best_score,\n",
    "            'best_steps_done': self.best_steps_done\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    def load_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        checkpoint = torch.load(filename, map_location=device.type)\n",
    "        self.dqn.load_state_dict(checkpoint['dqn'])\n",
    "        self.target.load_state_dict(checkpoint['target'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.global_steps = checkpoint['global_steps']\n",
    "        self.best_score = checkpoint['best_score']\n",
    "        self.best_steps_done = checkpoint['best_steps_done']\n",
    "        \n",
    "        \n",
    "    def play(self, human=True):\n",
    "        self.env.reset()\n",
    "        state = self.get_initial_state()\n",
    "        steps_done = 0\n",
    "        total_score = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "            self.dqn.eval()\n",
    "            action = self.dqn(state).cpu().max(1)[1]\n",
    "\n",
    "            for _ in range(self.frame_skipping):\n",
    "                if human:\n",
    "                    screen = self.env.game.render(mode='human')\n",
    "                _, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_state = self.env.get_screen()\n",
    "                self.state_buffer.append(next_state)\n",
    "                state = np.array(self.state_buffer)\n",
    "\n",
    "                total_score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            steps_done += 1\n",
    "            \n",
    "            if done:\n",
    "                self.dqn.train()\n",
    "                break\n",
    "        self.env.game.close()\n",
    "        return total_score, steps_done\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_screen()\n",
    "        \n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        [self.state_buffer.append(state) for _ in range(self.state_buffer_size)]\n",
    "        return np.array(self.state_buffer)\n",
    "    \n",
    "    def optimize(self):\n",
    "        tree_idx, transitions, ISWeights = self.memory.sample(n=self.batch_size)\n",
    "\n",
    "        states = torch.cat(transitions.state).to(device)\n",
    "        actions = torch.cat(transitions.action).to(device)\n",
    "        rewards = torch.cat(transitions.reward).to(device)\n",
    "        ISWeights = torch.FloatTensor(ISWeights, device=device)\n",
    "        \n",
    "        non_final_mask = torch.tensor(list(map(lambda s: s is not None, transitions.next_state)), \n",
    "                                      device=device, dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([s for s in transitions.next_state\n",
    "                                            if s is not None])\n",
    "        \n",
    "        states = states.view([self.batch_size, self.state_buffer_size, self.env.width, self.env.height])\n",
    "\n",
    "        q_values = self.dqn(states).gather(1, actions)\n",
    "        \n",
    "        target_values = torch.zeros(self.batch_size, device=device)\n",
    "        non_final_next_states = non_final_next_states.view([-1, self.state_buffer_size, self.env.width, self.env.height]).to(device)\n",
    "        target_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        expected_state_action_values = (target_values * self.gamma) + rewards\n",
    "        \n",
    "        # Update priority\n",
    "        absolute_errors = torch.abs(expected_state_action_values.unsqueeze(1) - q_values).detach()\n",
    "        self.memory.batch_update(tree_idx, absolute_errors)\n",
    "\n",
    "        loss = torch.mean(ISWeights * F.mse_loss(q_values, expected_state_action_values.unsqueeze(1)))\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        reward_score = int(torch.sum(rewards).cpu().detach().numpy())\n",
    "        \n",
    "        return loss.cpu().detach().numpy(), reward_score\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        eps = self.epsilon_stop + (self.epsilon_start - self.epsilon_stop) * np.exp(-self.global_steps/self.epsilon_decay)\n",
    "        if np.random.uniform() <= eps:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            with torch.no_grad():\n",
    "                return self.dqn(state).max(1)[1].view(-1,1).cpu(), eps\n",
    "        else:\n",
    "            sample_action = self.env.game.action_space.sample()\n",
    "            action = torch.LongTensor([[sample_action]])\n",
    "            return action, eps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        while True:\n",
    "            self.env.reset()\n",
    "            state = self.get_initial_state()\n",
    "            \n",
    "            losses = []\n",
    "            rewards = []\n",
    "            steps_done = 0\n",
    "            eps = self.epsilon_start\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                action, e = self.action(state)\n",
    "                eps = e\n",
    "                \n",
    "                for _ in range(self.frame_skipping):\n",
    "                    _, reward, done, _ = self.env.step(action.item())\n",
    "                    next_state = self.env.get_screen()\n",
    "                    self.state_buffer.append(next_state)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "            \n",
    "            \n",
    "                next_state = np.array(self.state_buffer)\n",
    "                \n",
    "                if done:\n",
    "                    self.memory.add(state, action, reward, None)\n",
    "                else:\n",
    "                    self.memory.add(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                if self.memory.is_ready():\n",
    "                    l, r = self.optimize()\n",
    "                    losses.append(l)\n",
    "                    rewards.append(r)\n",
    "\n",
    "                steps_done += 1\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                # update target network\n",
    "                if self.global_steps % self.target_update_interval == 0:\n",
    "                    print('update target network!')\n",
    "                    self.target.load_state_dict(self.dqn.state_dict())\n",
    "              \n",
    "                # Play\n",
    "                if self.global_steps % self.play_interval == 0:\n",
    "                    scores = []\n",
    "                    total_steps_done = []\n",
    "                    for _ in range(self.play_repeat):\n",
    "                        score, steps_done = self.play(human=True)\n",
    "                        scores.append(score)\n",
    "                        total_steps_done.append(steps_done)\n",
    "                        \n",
    "                    real_score = int(np.mean(scores))\n",
    "                    real_steps_done = int(np.mean(total_steps_done))\n",
    "\n",
    "                    if self.best_steps_done <= real_steps_done:\n",
    "                        self.best_score = real_score\n",
    "                        self.best_steps_done = real_steps_done\n",
    "\n",
    "                        self.save_checkpoint(\n",
    "                            filename=f'dqn_checkpoints/chkpoint_{self.best_steps_done}_{self.best_score}.pth')\n",
    "\n",
    "            if not np.isnan(np.mean(losses)):\n",
    "                print('global_steps:%d, episode_steps:%d, loss:%.6f, rewards:%d, eps:%.6f'%(self.global_steps, steps_done, np.mean(losses), np.mean(rewards), eps))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_steps:25, episode_steps:16, loss:0.988490, rewards:-16, eps:0.999762\n",
      "global_steps:34, episode_steps:9, loss:1.689945, rewards:-31, eps:0.999673\n",
      "global_steps:47, episode_steps:13, loss:1.084864, rewards:-26, eps:0.999545\n",
      "global_steps:52, episode_steps:5, loss:1.568050, rewards:-30, eps:0.999495\n",
      "global_steps:57, episode_steps:5, loss:1.897261, rewards:-35, eps:0.999446\n",
      "global_steps:62, episode_steps:5, loss:2.278837, rewards:-41, eps:0.999396\n",
      "global_steps:70, episode_steps:8, loss:1.497470, rewards:-30, eps:0.999317\n",
      "global_steps:90, episode_steps:20, loss:0.844229, rewards:-10, eps:0.999119\n",
      "global_steps:106, episode_steps:16, loss:1.193650, rewards:-14, eps:0.998961\n",
      "global_steps:122, episode_steps:16, loss:0.368180, rewards:-17, eps:0.998803\n",
      "global_steps:138, episode_steps:16, loss:0.585659, rewards:-16, eps:0.998645\n",
      "global_steps:154, episode_steps:16, loss:0.625450, rewards:-15, eps:0.998486\n",
      "global_steps:170, episode_steps:16, loss:0.285026, rewards:-13, eps:0.998328\n",
      "global_steps:186, episode_steps:16, loss:0.162304, rewards:-14, eps:0.998170\n",
      "global_steps:202, episode_steps:16, loss:0.059476, rewards:-14, eps:0.998012\n",
      "global_steps:218, episode_steps:16, loss:0.098334, rewards:-17, eps:0.997854\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fa6412a8ffcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# agent.get_initial_state()[0].shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-c51338dcbeac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-c51338dcbeac>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "agent = Agent() \n",
    "\n",
    "# agent.get_initial_state()[0].shape\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_checkpoint('dqn_checkpoints/exp_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 197)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # play\n",
    "# agent = Agent()\n",
    "# agent.load_checkpoint('dqn_checkpoints/chkpoint_771_76.pth')\n",
    "# agent.play(human=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
