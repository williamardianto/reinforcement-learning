{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pygame\n",
    "# pip install pygame\n",
    "\n",
    "# install ple\n",
    "# git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "# cd PyGame-Learning-Environment/\n",
    "# pip install -e .\n",
    "\n",
    "# install gym_ple\n",
    "# pip install gym_ple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Utils import Environment, Memory\n",
    "import os\n",
    " \n",
    "import matplotlib\n",
    "from matplotlib.pyplot import imshow, show\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, Box(512, 288, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2306e482ac8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.init()\n",
    "env.step(1)\n",
    "imshow(env.get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQRN, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=49, hidden_size=self.hidden_size, num_layers=1)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8192, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1) # (32, 64, 7, 7) -> (32, 64, 49)\n",
    "        x, (next_hidden_state, next_cell_state) = self.lstm(x, (hidden_state, cell_state))\n",
    "        x = x.view(x.size(0), -1) # (32, 64, 128) -> (32, 8192)\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        x = self.fc2(x)\n",
    "        return x , next_hidden_state, next_cell_state\n",
    "    \n",
    "    def init_states(self):\n",
    "        hidden_state = torch.zeros(1, 64, self.hidden_size, device=device)\n",
    "        cell_state = torch.zeros(1, 64, self.hidden_size, device=device)\n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqrn = DQRN()\n",
    "(h,c) = dqrn.init_states()\n",
    "x = torch.randn(32,4,84,84)\n",
    "# dqrn(x,h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.dqn = DQRN().to(device)\n",
    "        self.target = DQRN().to(device)\n",
    "        self.target.load_state_dict(self.dqn.state_dict())\n",
    "        self.target.eval()\n",
    "        \n",
    "        # For Optimization\n",
    "        self.dqn_hidden_state, self.dqn_cell_state = self.dqn.init_states()\n",
    "        self.target_hidden_state, self.target_cell_state = self.dqn.init_states()\n",
    "\n",
    "        # For Training Play\n",
    "        self.train_hidden_state, self.train_cell_state = self.dqn.init_states()\n",
    "\n",
    "        # For Validation Play\n",
    "        self.test_hidden_state, self.test_cell_state = self.dqn.init_states()\n",
    "\n",
    "        self.target_update_interval = 1000\n",
    "#         self.optimizer = optim.RMSprop(self.dqn.parameters())\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.0001)\n",
    "        \n",
    "        #play params\n",
    "        self.play_interval = 1000\n",
    "        self.play_repeat = 1\n",
    "        self.best_steps_done = 0\n",
    "        self.best_score = -1000\n",
    "        \n",
    "        self.seed = 111\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_decay = 100000\n",
    "        self.epsilon_stop = 0.01\n",
    "        self.batch_size = 32\n",
    "#         self.max_step = 1000\n",
    "        self.global_steps = 0\n",
    "        \n",
    "        self.memory = Memory(50000, batch_size=self.batch_size)\n",
    "\n",
    "        self.frame_skipping = 4\n",
    "        self.state_buffer_size = 4\n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        \n",
    "    def save_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        dirpath = os.path.dirname(filename)\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.mkdir(dirpath)\n",
    "\n",
    "        checkpoint = {\n",
    "            'dqn': self.dqn.state_dict(),\n",
    "            'target': self.target.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'global_steps': self.global_steps,\n",
    "            'best_score': self.best_score,\n",
    "            'best_steps_done': self.best_steps_done\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    def load_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        checkpoint = torch.load(filename, map_location=device.type)\n",
    "        self.dqn.load_state_dict(checkpoint['dqn'])\n",
    "        self.target.load_state_dict(checkpoint['target'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.global_steps = checkpoint['global_steps']\n",
    "        self.best_score = checkpoint['best_score']\n",
    "        self.best_steps_done = checkpoint['best_steps_done']\n",
    "        \n",
    "        \n",
    "    def play(self, human=True):\n",
    "        self.test_hidden_state, self.test_cell_state = self.dqn.init_states()\n",
    "        \n",
    "        self.env.reset()\n",
    "        state = self.get_initial_state()\n",
    "        steps_done = 0\n",
    "        total_score = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "            self.dqn.eval()\n",
    "            \n",
    "            dqn_pred, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(state, self.test_hidden_state, self.test_cell_state)\n",
    "            action = dqn_pred.cpu().max(1)[1]\n",
    "            \n",
    "            for _ in range(self.frame_skipping):\n",
    "                if human:\n",
    "                    screen = self.env.game.render(mode='human')\n",
    "                _, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_state = self.env.get_screen()\n",
    "                self.state_buffer.append(next_state)\n",
    "                state = np.array(self.state_buffer)\n",
    "\n",
    "                total_score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            steps_done += 1\n",
    "            \n",
    "            if done:\n",
    "                self.dqn.train()\n",
    "                break\n",
    "        self.env.game.close()\n",
    "        return total_score, steps_done\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_screen()\n",
    "        \n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        [self.state_buffer.append(state) for _ in range(self.state_buffer_size)]\n",
    "        return np.array(self.state_buffer)\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.dqn_hidden_state, self.dqn_cell_state = self.dqn.init_states()\n",
    "        self.target_hidden_state, self.target_cell_state = self.dqn.init_states()\n",
    "        \n",
    "        transitions = self.memory.sample()\n",
    "\n",
    "        states = torch.cat(transitions.state).to(device)\n",
    "        actions = torch.cat(transitions.action).to(device)\n",
    "        rewards = torch.cat(transitions.reward).to(device)\n",
    "        \n",
    "        non_final_mask = torch.tensor(list(map(lambda s: s is not None, transitions.next_state)), \n",
    "                                      device=device, dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([s for s in transitions.next_state\n",
    "                                            if s is not None])\n",
    "        \n",
    "        states = states.view([self.batch_size, self.state_buffer_size, self.env.width, self.env.height])\n",
    "        q_pred, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(states, self.dqn_hidden_state, self.dqn_cell_state)\n",
    "        q_values = q_pred.gather(1, actions)\n",
    "        \n",
    "        target_values = torch.zeros(self.batch_size, device=device)\n",
    "        non_final_next_states = non_final_next_states.view([-1, self.state_buffer_size, self.env.width, self.env.height]).to(device)\n",
    "        target_pred, self.target_hidden_state, self.target_cell_state = self.target(non_final_next_states,\n",
    "                                                                            self.target_hidden_state,\n",
    "                                                                            self.target_cell_state)\n",
    "        target_values[non_final_mask] = target_pred.max(1)[0].detach()\n",
    "        \n",
    "#         rewards.data.clamp_(-1, 1)\n",
    "        \n",
    "        expected_state_action_values = (target_values * self.gamma) + rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        reward_score = int(torch.sum(rewards).cpu().detach().numpy())\n",
    "        \n",
    "        return loss.cpu().detach().numpy(), reward_score\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        eps = self.epsilon_stop + (self.epsilon_start - self.epsilon_stop) * np.exp(-self.global_steps/self.epsilon_decay)\n",
    "        if np.random.uniform() <= eps:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            with torch.no_grad():\n",
    "                action, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(state, self.train_hidden_state, self.train_cell_state)\n",
    "                action = action.max(1)[1].view(-1,1).cpu()\n",
    "                return action, eps\n",
    "        else:\n",
    "            sample_action = self.env.game.action_space.sample()\n",
    "            action = torch.LongTensor([[sample_action]])\n",
    "            return action, eps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        while True:\n",
    "            self.train_hidden_state, self.train_cell_state = self.dqn.init_states()\n",
    "            \n",
    "            self.env.reset()\n",
    "            state = self.get_initial_state()\n",
    "            \n",
    "            losses = []\n",
    "            rewards = []\n",
    "            steps_done = 0\n",
    "            eps = self.epsilon_start\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                action, e = self.action(state)\n",
    "                eps = e\n",
    "                \n",
    "                for _ in range(self.frame_skipping):\n",
    "                    _, reward, done, _ = self.env.step(action.item())\n",
    "                    next_state = self.env.get_screen()\n",
    "                    self.state_buffer.append(next_state)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "            \n",
    "            \n",
    "                next_state = np.array(self.state_buffer)\n",
    "                \n",
    "                if done:\n",
    "                    self.memory.add(state, action, reward, None)\n",
    "                else:\n",
    "                    self.memory.add(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                if self.memory.is_ready():\n",
    "                    l, r = self.optimize()\n",
    "                    losses.append(l)\n",
    "                    rewards.append(r)\n",
    "\n",
    "                steps_done += 1\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                # update target network\n",
    "                if self.global_steps % self.target_update_interval == 0:\n",
    "                    print('update target network!')\n",
    "                    self.target.load_state_dict(self.dqn.state_dict())\n",
    "              \n",
    "                # Play\n",
    "                if self.global_steps % self.play_interval == 0:\n",
    "                    scores = []\n",
    "                    total_steps_done = []\n",
    "                    for _ in range(self.play_repeat):\n",
    "                        score, steps_done = self.play(human=True)\n",
    "                        scores.append(score)\n",
    "                        total_steps_done.append(steps_done)\n",
    "                        \n",
    "                    real_score = int(np.mean(scores))\n",
    "                    real_steps_done = int(np.mean(total_steps_done))\n",
    "\n",
    "                    if self.best_steps_done <= real_steps_done:\n",
    "                        self.best_score = real_score\n",
    "                        self.best_steps_done = real_steps_done\n",
    "\n",
    "                        self.save_checkpoint(\n",
    "                            filename=f'dqrn_checkpoints/chkpoint_{self.best_steps_done}_{self.best_score}.pth')\n",
    "\n",
    "            if not np.isnan(np.mean(losses)):\n",
    "                print('global_steps:%d, episode_steps:%d, loss:%.6f, rewards:%d, eps:%.6f'%(self.global_steps, steps_done, np.mean(losses), np.mean(rewards), eps))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "# agent = Agent() \n",
    "# agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_checkpoint('dqrn_checkpoints/exp_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.0, 194)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play\n",
    "# agent = Agent()\n",
    "# agent.load_checkpoint('dqrn_checkpoints/exp_01.pth')\n",
    "# agent.play(human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
