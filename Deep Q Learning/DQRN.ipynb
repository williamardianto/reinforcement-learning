{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pygame\n",
    "# pip install pygame\n",
    "\n",
    "# install ple\n",
    "# git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "# cd PyGame-Learning-Environment/\n",
    "# pip install -e .\n",
    "\n",
    "# install gym_ple\n",
    "# pip install gym_ple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Utils import Environment, Memory\n",
    "import os\n",
    " \n",
    "import matplotlib\n",
    "from matplotlib.pyplot import imshow, show\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, Box(512, 288, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c2b4bcba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.init()\n",
    "env.step(1)\n",
    "imshow(env.get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQRN, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=49, hidden_size=self.hidden_size, num_layers=1)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8192, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1) # (32, 64, 7, 7) -> (32, 64, 49)\n",
    "        x, (next_hidden_state, next_cell_state) = self.lstm(x, (hidden_state, cell_state))\n",
    "        x = x.view(x.size(0), -1) # (32, 64, 128) -> (32, 8192)\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        x = self.fc2(x)\n",
    "        return x , next_hidden_state, next_cell_state\n",
    "    \n",
    "    def init_states(self):\n",
    "        hidden_state = torch.zeros(1, 64, self.hidden_size, device=device)\n",
    "        cell_state = torch.zeros(1, 64, self.hidden_size, device=device)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "    def reset_states(self, hidden_state, cell_state):\n",
    "        hidden_state[:, :, :] = 0\n",
    "        cell_state[:, :, :] = 0\n",
    "        return hidden_state.detach(), cell_state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqrn = DQRN()\n",
    "(h,c) = dqrn.init_states()\n",
    "x = torch.randn(32,4,84,84)\n",
    "# dqrn(x,h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.dqn = DQRN().to(device)\n",
    "        self.target = DQRN().to(device)\n",
    "        self.target.load_state_dict(self.dqn.state_dict())\n",
    "        self.target.eval()\n",
    "        \n",
    "        # For Optimization\n",
    "        self.dqn_hidden_state, self.dqn_cell_state = self.dqn.init_states()\n",
    "        self.target_hidden_state, self.target_cell_state = self.dqn.init_states()\n",
    "\n",
    "        # For Training Play\n",
    "        self.train_hidden_state, self.train_cell_state = self.dqn.init_states()\n",
    "\n",
    "        # For Validation Play\n",
    "        self.test_hidden_state, self.test_cell_state = self.dqn.init_states()\n",
    "\n",
    "        self.target_update_interval = 1000\n",
    "#         self.optimizer = optim.RMSprop(self.dqn.parameters())\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.0001)\n",
    "        \n",
    "        #play params\n",
    "        self.play_interval = 1000\n",
    "        self.play_repeat = 1\n",
    "        self.best_steps_done = 0\n",
    "        self.best_score = -1000\n",
    "        \n",
    "        self.seed = 111\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_decay = 100000\n",
    "        self.epsilon_stop = 0.01\n",
    "        self.batch_size = 32\n",
    "#         self.max_step = 1000\n",
    "        self.global_steps = 0\n",
    "        \n",
    "        self.memory = Memory(50000, batch_size=self.batch_size)\n",
    "\n",
    "        self.frame_skipping = 4\n",
    "        self.state_buffer_size = 4\n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        \n",
    "    def save_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        dirpath = os.path.dirname(filename)\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.mkdir(dirpath)\n",
    "\n",
    "        checkpoint = {\n",
    "            'dqn': self.dqn.state_dict(),\n",
    "            'target': self.target.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'global_steps': self.global_steps,\n",
    "            'best_score': self.best_score,\n",
    "            'best_steps_done': self.best_steps_done\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    def load_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        checkpoint = torch.load(filename, map_location=device.type)\n",
    "        self.dqn.load_state_dict(checkpoint['dqn'])\n",
    "        self.target.load_state_dict(checkpoint['target'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.global_steps = checkpoint['global_steps']\n",
    "        self.best_score = checkpoint['best_score']\n",
    "        self.best_steps_done = checkpoint['best_steps_done']\n",
    "        \n",
    "        \n",
    "    def play(self, human=True):\n",
    "        self.test_hidden_state, self.test_cell_state = self.dqn.reset_states(self.test_hidden_state,\n",
    "                                                                     self.test_cell_state)\n",
    "        \n",
    "        self.env.reset()\n",
    "        state = self.get_initial_state()\n",
    "        steps_done = 0\n",
    "        total_score = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "            self.dqn.eval()\n",
    "            \n",
    "            dqn_pred, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(state, self.test_hidden_state, self.test_cell_state)\n",
    "            action = dqn_pred.cpu().max(1)[1]\n",
    "            \n",
    "            for _ in range(self.frame_skipping):\n",
    "                if human:\n",
    "                    screen = self.env.game.render(mode='human')\n",
    "                _, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_state = self.env.get_screen()\n",
    "                self.state_buffer.append(next_state)\n",
    "                state = np.array(self.state_buffer)\n",
    "\n",
    "                total_score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            steps_done += 1\n",
    "            \n",
    "            if done:\n",
    "                self.dqn.train()\n",
    "                break\n",
    "        self.env.game.close()\n",
    "        return total_score, steps_done\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_screen()\n",
    "        \n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        [self.state_buffer.append(state) for _ in range(self.state_buffer_size)]\n",
    "        return np.array(self.state_buffer)\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.dqn_hidden_state, self.dqn_cell_state = self.dqn.reset_states(self.dqn_hidden_state,\n",
    "                                                                           self.dqn_cell_state)\n",
    "        self.target_hidden_state, self.target_cell_state = self.dqn.reset_states(self.target_hidden_state,\n",
    "                                                                                     self.target_cell_state)\n",
    "        \n",
    "        transitions = self.memory.sample()\n",
    "\n",
    "        states = torch.cat(transitions.state).to(device)\n",
    "        actions = torch.cat(transitions.action).to(device)\n",
    "        rewards = torch.cat(transitions.reward).to(device)\n",
    "        \n",
    "        non_final_mask = torch.tensor(list(map(lambda s: s is not None, transitions.next_state)), \n",
    "                                      device=device, dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([s for s in transitions.next_state\n",
    "                                            if s is not None])\n",
    "        \n",
    "        states = states.view([self.batch_size, self.state_buffer_size, self.env.width, self.env.height])\n",
    "        q_pred, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(states, self.dqn_hidden_state, self.dqn_cell_state)\n",
    "        q_values = q_pred.gather(1, actions)\n",
    "        \n",
    "        target_values = torch.zeros(self.batch_size, device=device)\n",
    "        non_final_next_states = non_final_next_states.view([-1, self.state_buffer_size, self.env.width, self.env.height]).to(device)\n",
    "        target_pred, self.target_hidden_state, self.target_cell_state = self.target(non_final_next_states,\n",
    "                                                                            self.target_hidden_state,\n",
    "                                                                            self.target_cell_state)\n",
    "        target_values[non_final_mask] = target_pred.max(1)[0].detach()\n",
    "        \n",
    "        rewards.data.clamp_(-1, 1)\n",
    "        \n",
    "        expected_state_action_values = (target_values * self.gamma) + rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        reward_score = int(torch.sum(rewards).cpu().detach().numpy())\n",
    "        \n",
    "        return loss.cpu().detach().numpy(), reward_score\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        eps = self.epsilon_stop + (self.epsilon_start - self.epsilon_stop) * np.exp(-self.global_steps/self.epsilon_decay)\n",
    "        if np.random.uniform() <= eps:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            with torch.no_grad():\n",
    "                action, self.dqn_hidden_state, self.dqn_cell_state = self.dqn(state, self.train_hidden_state, self.train_cell_state)\n",
    "                action = action.max(1)[1].view(-1,1).cpu()\n",
    "                return action, eps\n",
    "        else:\n",
    "            sample_action = self.env.game.action_space.sample()\n",
    "            action = torch.LongTensor([[sample_action]])\n",
    "            return action, eps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        while True:\n",
    "            self.train_hidden_state, self.train_cell_state = self.dqn.reset_states(self.train_hidden_state,\n",
    "                                                                       self.train_cell_state)\n",
    "            \n",
    "            self.env.reset()\n",
    "            state = self.get_initial_state()\n",
    "            \n",
    "            losses = []\n",
    "            rewards = []\n",
    "            steps_done = 0\n",
    "            eps = self.epsilon_start\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                action, e = self.action(state)\n",
    "                eps = e\n",
    "                \n",
    "                for _ in range(self.frame_skipping):\n",
    "                    _, reward, done, _ = self.env.step(action.item())\n",
    "                    next_state = self.env.get_screen()\n",
    "                    self.state_buffer.append(next_state)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "            \n",
    "            \n",
    "                next_state = np.array(self.state_buffer)\n",
    "                \n",
    "                if done:\n",
    "                    self.memory.add(state, action, reward, None)\n",
    "                else:\n",
    "                    self.memory.add(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                if self.memory.is_ready():\n",
    "                    l, r = self.optimize()\n",
    "                    losses.append(l)\n",
    "                    rewards.append(r)\n",
    "\n",
    "                steps_done += 1\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                # update target network\n",
    "                if self.global_steps % self.target_update_interval == 0:\n",
    "                    print('update target network!')\n",
    "                    self.target.load_state_dict(self.dqn.state_dict())\n",
    "              \n",
    "                # Play\n",
    "                if self.global_steps % self.play_interval == 0:\n",
    "                    scores = []\n",
    "                    total_steps_done = []\n",
    "                    for _ in range(self.play_repeat):\n",
    "                        score, steps_done = self.play(human=True)\n",
    "                        scores.append(score)\n",
    "                        total_steps_done.append(steps_done)\n",
    "                        \n",
    "                    real_score = int(np.mean(scores))\n",
    "                    real_steps_done = int(np.mean(total_steps_done))\n",
    "\n",
    "                    if self.best_steps_done <= real_steps_done:\n",
    "                        self.best_score = real_score\n",
    "                        self.best_steps_done = real_steps_done\n",
    "\n",
    "                        self.save_checkpoint(\n",
    "                            filename=f'dqn_checkpoints/chkpoint_{self.best_steps_done}_{self.best_score}.pth')\n",
    "\n",
    "            if not np.isnan(np.mean(losses)):\n",
    "                print('global_steps:%d, episode_steps:%d, loss:%.6f, rewards:%d, eps:%.6f'%(self.global_steps, steps_done, np.mean(losses), np.mean(rewards), eps))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_steps:45, episode_steps:13, loss:0.020992, rewards:-1, eps:0.999564\n",
      "global_steps:61, episode_steps:16, loss:0.012663, rewards:-1, eps:0.999406\n",
      "global_steps:77, episode_steps:16, loss:0.008840, rewards:-1, eps:0.999248\n",
      "global_steps:93, episode_steps:16, loss:0.004432, rewards:-1, eps:0.999090\n",
      "global_steps:109, episode_steps:16, loss:0.004154, rewards:-2, eps:0.998931\n",
      "global_steps:125, episode_steps:16, loss:0.002543, rewards:-1, eps:0.998773\n",
      "global_steps:141, episode_steps:16, loss:0.001811, rewards:-1, eps:0.998615\n",
      "global_steps:157, episode_steps:16, loss:0.001102, rewards:-1, eps:0.998457\n",
      "global_steps:173, episode_steps:16, loss:0.001340, rewards:-1, eps:0.998299\n",
      "global_steps:189, episode_steps:16, loss:0.001356, rewards:-1, eps:0.998141\n",
      "global_steps:205, episode_steps:16, loss:0.000803, rewards:-1, eps:0.997982\n",
      "global_steps:221, episode_steps:16, loss:0.000857, rewards:-2, eps:0.997824\n",
      "global_steps:237, episode_steps:16, loss:0.000541, rewards:-1, eps:0.997666\n",
      "global_steps:253, episode_steps:16, loss:0.000650, rewards:-2, eps:0.997508\n",
      "global_steps:269, episode_steps:16, loss:0.000527, rewards:-1, eps:0.997350\n",
      "global_steps:285, episode_steps:16, loss:0.000523, rewards:-2, eps:0.997192\n",
      "global_steps:301, episode_steps:16, loss:0.000539, rewards:-2, eps:0.997034\n",
      "global_steps:317, episode_steps:16, loss:0.000566, rewards:-2, eps:0.996877\n",
      "global_steps:333, episode_steps:16, loss:0.000481, rewards:-2, eps:0.996719\n",
      "global_steps:349, episode_steps:16, loss:0.000336, rewards:-2, eps:0.996561\n",
      "global_steps:365, episode_steps:16, loss:0.000393, rewards:-2, eps:0.996403\n",
      "global_steps:381, episode_steps:16, loss:0.000491, rewards:-2, eps:0.996245\n",
      "global_steps:397, episode_steps:16, loss:0.000273, rewards:-2, eps:0.996087\n",
      "global_steps:413, episode_steps:16, loss:0.000332, rewards:-2, eps:0.995930\n",
      "global_steps:429, episode_steps:16, loss:0.000307, rewards:-2, eps:0.995772\n",
      "global_steps:445, episode_steps:16, loss:0.000696, rewards:-2, eps:0.995614\n",
      "global_steps:461, episode_steps:16, loss:0.000215, rewards:-1, eps:0.995456\n",
      "global_steps:477, episode_steps:16, loss:0.000230, rewards:-2, eps:0.995299\n",
      "global_steps:493, episode_steps:16, loss:0.000434, rewards:-2, eps:0.995141\n",
      "global_steps:509, episode_steps:16, loss:0.000333, rewards:-1, eps:0.994984\n",
      "global_steps:525, episode_steps:16, loss:0.000219, rewards:-1, eps:0.994826\n",
      "global_steps:541, episode_steps:16, loss:0.000188, rewards:-1, eps:0.994668\n",
      "global_steps:557, episode_steps:16, loss:0.000482, rewards:-2, eps:0.994511\n",
      "global_steps:573, episode_steps:16, loss:0.000462, rewards:-2, eps:0.994353\n",
      "global_steps:589, episode_steps:16, loss:0.000363, rewards:-1, eps:0.994196\n",
      "global_steps:605, episode_steps:16, loss:0.000262, rewards:-1, eps:0.994038\n",
      "global_steps:621, episode_steps:16, loss:0.000162, rewards:-2, eps:0.993881\n",
      "global_steps:637, episode_steps:16, loss:0.000154, rewards:-1, eps:0.993724\n",
      "global_steps:653, episode_steps:16, loss:0.000163, rewards:-2, eps:0.993566\n",
      "global_steps:669, episode_steps:16, loss:0.000108, rewards:-2, eps:0.993409\n",
      "global_steps:685, episode_steps:16, loss:0.000139, rewards:-1, eps:0.993252\n",
      "global_steps:701, episode_steps:16, loss:0.000244, rewards:-1, eps:0.993094\n",
      "global_steps:717, episode_steps:16, loss:0.000485, rewards:-1, eps:0.992937\n",
      "global_steps:733, episode_steps:16, loss:0.000278, rewards:-1, eps:0.992780\n",
      "global_steps:749, episode_steps:16, loss:0.000255, rewards:-2, eps:0.992622\n",
      "global_steps:765, episode_steps:16, loss:0.000225, rewards:-1, eps:0.992465\n",
      "global_steps:781, episode_steps:16, loss:0.000187, rewards:-2, eps:0.992308\n",
      "global_steps:797, episode_steps:16, loss:0.000102, rewards:-1, eps:0.992151\n",
      "global_steps:813, episode_steps:16, loss:0.000128, rewards:-1, eps:0.991994\n",
      "global_steps:829, episode_steps:16, loss:0.000158, rewards:-1, eps:0.991837\n",
      "global_steps:845, episode_steps:16, loss:0.000099, rewards:-1, eps:0.991680\n",
      "global_steps:861, episode_steps:16, loss:0.000231, rewards:-2, eps:0.991523\n",
      "global_steps:877, episode_steps:16, loss:0.000134, rewards:-1, eps:0.991365\n",
      "global_steps:893, episode_steps:16, loss:0.000172, rewards:-2, eps:0.991208\n",
      "global_steps:909, episode_steps:16, loss:0.000242, rewards:-1, eps:0.991051\n",
      "global_steps:925, episode_steps:16, loss:0.000132, rewards:-1, eps:0.990895\n",
      "global_steps:941, episode_steps:16, loss:0.000196, rewards:-2, eps:0.990738\n",
      "global_steps:957, episode_steps:16, loss:0.000206, rewards:-2, eps:0.990581\n",
      "global_steps:973, episode_steps:16, loss:0.000358, rewards:-2, eps:0.990424\n",
      "global_steps:989, episode_steps:16, loss:0.000169, rewards:-1, eps:0.990267\n",
      "update target network!\n",
      "global_steps:1001, episode_steps:17, loss:0.001530, rewards:-2, eps:0.990149\n",
      "global_steps:1017, episode_steps:16, loss:0.019225, rewards:-2, eps:0.989993\n",
      "global_steps:1033, episode_steps:16, loss:0.003390, rewards:-2, eps:0.989836\n",
      "global_steps:1049, episode_steps:16, loss:0.001864, rewards:-2, eps:0.989679\n",
      "global_steps:1065, episode_steps:16, loss:0.001167, rewards:-1, eps:0.989522\n",
      "global_steps:1081, episode_steps:16, loss:0.001098, rewards:-2, eps:0.989366\n",
      "global_steps:1097, episode_steps:16, loss:0.000665, rewards:-2, eps:0.989209\n",
      "global_steps:1113, episode_steps:16, loss:0.000307, rewards:-2, eps:0.989052\n",
      "global_steps:1129, episode_steps:16, loss:0.000345, rewards:-1, eps:0.988896\n",
      "global_steps:1145, episode_steps:16, loss:0.000277, rewards:-1, eps:0.988739\n",
      "global_steps:1162, episode_steps:17, loss:0.000300, rewards:-1, eps:0.988573\n",
      "global_steps:1178, episode_steps:16, loss:0.002423, rewards:-2, eps:0.988416\n",
      "global_steps:1194, episode_steps:16, loss:0.001024, rewards:-1, eps:0.988259\n",
      "global_steps:1210, episode_steps:16, loss:0.000783, rewards:-2, eps:0.988103\n",
      "global_steps:1226, episode_steps:16, loss:0.002370, rewards:-2, eps:0.987946\n",
      "global_steps:1242, episode_steps:16, loss:0.000646, rewards:-1, eps:0.987790\n",
      "global_steps:1258, episode_steps:16, loss:0.001505, rewards:-1, eps:0.987634\n",
      "global_steps:1274, episode_steps:16, loss:0.000644, rewards:-2, eps:0.987477\n",
      "global_steps:1290, episode_steps:16, loss:0.001561, rewards:-2, eps:0.987321\n",
      "global_steps:1306, episode_steps:16, loss:0.000443, rewards:-2, eps:0.987164\n",
      "global_steps:1322, episode_steps:16, loss:0.000437, rewards:-1, eps:0.987008\n",
      "global_steps:1338, episode_steps:16, loss:0.000245, rewards:-2, eps:0.986852\n",
      "global_steps:1354, episode_steps:16, loss:0.000208, rewards:-1, eps:0.986696\n",
      "global_steps:1370, episode_steps:16, loss:0.000379, rewards:-2, eps:0.986539\n",
      "global_steps:1386, episode_steps:16, loss:0.000210, rewards:-1, eps:0.986383\n",
      "global_steps:1402, episode_steps:16, loss:0.000985, rewards:-2, eps:0.986227\n",
      "global_steps:1418, episode_steps:16, loss:0.000934, rewards:-1, eps:0.986071\n",
      "global_steps:1434, episode_steps:16, loss:0.000599, rewards:-1, eps:0.985914\n",
      "global_steps:1454, episode_steps:20, loss:0.000536, rewards:-2, eps:0.985719\n",
      "global_steps:1470, episode_steps:16, loss:0.000762, rewards:-1, eps:0.985563\n",
      "global_steps:1486, episode_steps:16, loss:0.002464, rewards:-2, eps:0.985407\n",
      "global_steps:1502, episode_steps:16, loss:0.001495, rewards:-1, eps:0.985251\n",
      "global_steps:1518, episode_steps:16, loss:0.001342, rewards:-1, eps:0.985095\n",
      "global_steps:1534, episode_steps:16, loss:0.000459, rewards:-1, eps:0.984939\n",
      "global_steps:1550, episode_steps:16, loss:0.000687, rewards:-1, eps:0.984783\n",
      "global_steps:1566, episode_steps:16, loss:0.000585, rewards:-1, eps:0.984627\n",
      "global_steps:1582, episode_steps:16, loss:0.000447, rewards:-1, eps:0.984471\n",
      "global_steps:1601, episode_steps:19, loss:0.000371, rewards:-2, eps:0.984286\n",
      "global_steps:1617, episode_steps:16, loss:0.001335, rewards:-2, eps:0.984130\n",
      "global_steps:1633, episode_steps:16, loss:0.000609, rewards:-2, eps:0.983974\n",
      "global_steps:1658, episode_steps:25, loss:0.001404, rewards:-1, eps:0.983731\n",
      "global_steps:1674, episode_steps:16, loss:0.001200, rewards:-1, eps:0.983575\n",
      "global_steps:1690, episode_steps:16, loss:0.001425, rewards:-1, eps:0.983419\n",
      "global_steps:1706, episode_steps:16, loss:0.001626, rewards:-1, eps:0.983264\n",
      "global_steps:1722, episode_steps:16, loss:0.003002, rewards:-1, eps:0.983108\n",
      "global_steps:1739, episode_steps:17, loss:0.001766, rewards:-2, eps:0.982942\n",
      "global_steps:1755, episode_steps:16, loss:0.003727, rewards:-1, eps:0.982787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_steps:1771, episode_steps:16, loss:0.000707, rewards:-1, eps:0.982631\n",
      "global_steps:1787, episode_steps:16, loss:0.001513, rewards:-1, eps:0.982476\n",
      "global_steps:1803, episode_steps:16, loss:0.002116, rewards:-1, eps:0.982320\n",
      "global_steps:1819, episode_steps:16, loss:0.001492, rewards:-2, eps:0.982164\n",
      "global_steps:1835, episode_steps:16, loss:0.001039, rewards:-1, eps:0.982009\n",
      "global_steps:1860, episode_steps:25, loss:0.000893, rewards:-1, eps:0.981766\n",
      "global_steps:1876, episode_steps:16, loss:0.000535, rewards:-1, eps:0.981610\n",
      "global_steps:1892, episode_steps:16, loss:0.001821, rewards:-2, eps:0.981455\n",
      "global_steps:1908, episode_steps:16, loss:0.000856, rewards:-2, eps:0.981300\n",
      "global_steps:1924, episode_steps:16, loss:0.000462, rewards:-2, eps:0.981144\n",
      "global_steps:1940, episode_steps:16, loss:0.002171, rewards:-1, eps:0.980989\n",
      "global_steps:1956, episode_steps:16, loss:0.000663, rewards:-1, eps:0.980833\n",
      "global_steps:1972, episode_steps:16, loss:0.000467, rewards:-1, eps:0.980678\n",
      "global_steps:1988, episode_steps:16, loss:0.001569, rewards:-2, eps:0.980523\n",
      "update target network!\n",
      "global_steps:2001, episode_steps:17, loss:0.002056, rewards:-2, eps:0.980397\n",
      "global_steps:2017, episode_steps:16, loss:0.016501, rewards:-1, eps:0.980241\n",
      "global_steps:2036, episode_steps:19, loss:0.004373, rewards:-2, eps:0.980057\n",
      "global_steps:2052, episode_steps:16, loss:0.002840, rewards:-1, eps:0.979902\n",
      "global_steps:2068, episode_steps:16, loss:0.002482, rewards:-1, eps:0.979747\n",
      "global_steps:2084, episode_steps:16, loss:0.002123, rewards:-2, eps:0.979592\n",
      "global_steps:2100, episode_steps:16, loss:0.001552, rewards:-1, eps:0.979436\n",
      "global_steps:2116, episode_steps:16, loss:0.001739, rewards:-2, eps:0.979281\n",
      "global_steps:2132, episode_steps:16, loss:0.001031, rewards:-1, eps:0.979126\n",
      "global_steps:2148, episode_steps:16, loss:0.001565, rewards:-2, eps:0.978971\n",
      "global_steps:2164, episode_steps:16, loss:0.001366, rewards:-2, eps:0.978816\n",
      "global_steps:2180, episode_steps:16, loss:0.001068, rewards:-2, eps:0.978661\n",
      "global_steps:2196, episode_steps:16, loss:0.000688, rewards:-1, eps:0.978506\n",
      "global_steps:2212, episode_steps:16, loss:0.000451, rewards:-1, eps:0.978351\n",
      "global_steps:2228, episode_steps:16, loss:0.000649, rewards:-1, eps:0.978196\n",
      "global_steps:2244, episode_steps:16, loss:0.001089, rewards:-1, eps:0.978041\n",
      "global_steps:2261, episode_steps:17, loss:0.000680, rewards:-1, eps:0.977877\n",
      "global_steps:2277, episode_steps:16, loss:0.000711, rewards:-2, eps:0.977722\n",
      "global_steps:2293, episode_steps:16, loss:0.000765, rewards:-2, eps:0.977567\n",
      "global_steps:2309, episode_steps:16, loss:0.001619, rewards:-2, eps:0.977412\n",
      "global_steps:2328, episode_steps:19, loss:0.000930, rewards:-2, eps:0.977229\n",
      "global_steps:2344, episode_steps:16, loss:0.000604, rewards:-2, eps:0.977074\n",
      "global_steps:2360, episode_steps:16, loss:0.001222, rewards:-2, eps:0.976919\n",
      "global_steps:2385, episode_steps:25, loss:0.001394, rewards:-2, eps:0.976678\n",
      "global_steps:2401, episode_steps:16, loss:0.000706, rewards:-1, eps:0.976523\n",
      "global_steps:2417, episode_steps:16, loss:0.000496, rewards:-2, eps:0.976368\n",
      "global_steps:2433, episode_steps:16, loss:0.002638, rewards:-1, eps:0.976214\n",
      "global_steps:2458, episode_steps:25, loss:0.001401, rewards:-2, eps:0.975972\n",
      "global_steps:2474, episode_steps:16, loss:0.000983, rewards:-1, eps:0.975818\n",
      "global_steps:2490, episode_steps:16, loss:0.001438, rewards:-2, eps:0.975663\n",
      "global_steps:2515, episode_steps:25, loss:0.001430, rewards:-1, eps:0.975422\n",
      "global_steps:2531, episode_steps:16, loss:0.002919, rewards:-1, eps:0.975267\n",
      "global_steps:2565, episode_steps:34, loss:0.002284, rewards:-1, eps:0.974939\n",
      "global_steps:2590, episode_steps:25, loss:0.002648, rewards:-1, eps:0.974698\n",
      "global_steps:2606, episode_steps:16, loss:0.001220, rewards:-2, eps:0.974544\n",
      "global_steps:2622, episode_steps:16, loss:0.002205, rewards:-1, eps:0.974389\n",
      "global_steps:2647, episode_steps:25, loss:0.001957, rewards:-1, eps:0.974148\n",
      "global_steps:2663, episode_steps:16, loss:0.001013, rewards:-1, eps:0.973994\n",
      "global_steps:2679, episode_steps:16, loss:0.001610, rewards:-1, eps:0.973840\n",
      "global_steps:2704, episode_steps:25, loss:0.001546, rewards:-1, eps:0.973599\n",
      "global_steps:2720, episode_steps:16, loss:0.001660, rewards:-1, eps:0.973445\n",
      "global_steps:2745, episode_steps:25, loss:0.001250, rewards:-2, eps:0.973204\n",
      "global_steps:2761, episode_steps:16, loss:0.001101, rewards:-2, eps:0.973050\n",
      "global_steps:2777, episode_steps:16, loss:0.000942, rewards:-1, eps:0.972896\n",
      "global_steps:2802, episode_steps:25, loss:0.000858, rewards:-1, eps:0.972655\n",
      "global_steps:2827, episode_steps:25, loss:0.000730, rewards:-2, eps:0.972414\n",
      "global_steps:2843, episode_steps:16, loss:0.000642, rewards:-1, eps:0.972260\n",
      "global_steps:2859, episode_steps:16, loss:0.002478, rewards:-2, eps:0.972106\n",
      "global_steps:2875, episode_steps:16, loss:0.001200, rewards:-2, eps:0.971952\n",
      "global_steps:2891, episode_steps:16, loss:0.001107, rewards:-1, eps:0.971798\n",
      "global_steps:2907, episode_steps:16, loss:0.001567, rewards:-1, eps:0.971645\n",
      "global_steps:2923, episode_steps:16, loss:0.001320, rewards:-2, eps:0.971491\n",
      "global_steps:2948, episode_steps:25, loss:0.000965, rewards:-2, eps:0.971250\n",
      "global_steps:2964, episode_steps:16, loss:0.001079, rewards:-2, eps:0.971097\n",
      "global_steps:2980, episode_steps:16, loss:0.001368, rewards:-2, eps:0.970943\n",
      "global_steps:2996, episode_steps:16, loss:0.001051, rewards:-1, eps:0.970789\n",
      "update target network!\n",
      "global_steps:3001, episode_steps:21, loss:0.010128, rewards:-1, eps:0.970741\n",
      "global_steps:3017, episode_steps:16, loss:0.018407, rewards:-2, eps:0.970587\n",
      "global_steps:3037, episode_steps:20, loss:0.006305, rewards:-2, eps:0.970395\n",
      "global_steps:3053, episode_steps:16, loss:0.002443, rewards:-1, eps:0.970242\n",
      "global_steps:3078, episode_steps:25, loss:0.003252, rewards:-1, eps:0.970002\n",
      "global_steps:3094, episode_steps:16, loss:0.002242, rewards:-1, eps:0.969848\n",
      "global_steps:3110, episode_steps:16, loss:0.001333, rewards:-1, eps:0.969694\n",
      "global_steps:3126, episode_steps:16, loss:0.006193, rewards:-1, eps:0.969541\n",
      "global_steps:3152, episode_steps:26, loss:0.002825, rewards:-1, eps:0.969291\n",
      "global_steps:3168, episode_steps:16, loss:0.002793, rewards:-1, eps:0.969138\n",
      "global_steps:3184, episode_steps:16, loss:0.002935, rewards:-1, eps:0.968985\n",
      "global_steps:3200, episode_steps:16, loss:0.001521, rewards:-1, eps:0.968831\n",
      "global_steps:3216, episode_steps:16, loss:0.002069, rewards:-2, eps:0.968678\n",
      "global_steps:3232, episode_steps:16, loss:0.002473, rewards:-1, eps:0.968524\n",
      "global_steps:3248, episode_steps:16, loss:0.002354, rewards:-2, eps:0.968371\n",
      "global_steps:3264, episode_steps:16, loss:0.003778, rewards:-1, eps:0.968218\n",
      "global_steps:3298, episode_steps:34, loss:0.002173, rewards:-1, eps:0.967892\n",
      "global_steps:3314, episode_steps:16, loss:0.002569, rewards:-1, eps:0.967739\n",
      "global_steps:3330, episode_steps:16, loss:0.002250, rewards:-1, eps:0.967585\n",
      "global_steps:3355, episode_steps:25, loss:0.001944, rewards:-2, eps:0.967346\n",
      "global_steps:3371, episode_steps:16, loss:0.001975, rewards:-2, eps:0.967193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-82b1e98070f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-740d61146abc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skipping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/reinforcement-learning/Deep Q Learning/Utils.py\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/gym_ple/ple_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/gym_ple/ple_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimage_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfliplr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Hack to fix the rotated image returned by ple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage_rotated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/reinforcement-learning/Deep Q Learning/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36mgetScreenRGB\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetScreenGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/reinforcement-learning/Deep Q Learning/PyGame-Learning-Environment/ple/games/base/pygamewrapper.py\u001b[0m in \u001b[0;36mgetScreenRGB\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         return pygame.surfarray.array3d(\n\u001b[0;32m--> 101\u001b[0;31m             pygame.display.get_surface()).astype(np.uint8)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/pygame/surfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    127\u001b[0m     method).\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpixels3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dlenv/lib/python3.6/site-packages/pygame/_numpysurfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0msurface_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "agent = Agent() \n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_checkpoint('dqn_checkpoints/exp_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.0, 194)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play\n",
    "# agent = Agent()\n",
    "# agent.load_checkpoint('dqn_checkpoints/exp_01.pth')\n",
    "# agent.play(human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
