{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pygame\n",
    "# pip install pygame\n",
    "\n",
    "# install ple\n",
    "# git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "# cd PyGame-Learning-Environment/\n",
    "# pip install -e .\n",
    "\n",
    "# install gym_ple\n",
    "# pip install gym_ple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Utils import Environment, Memory\n",
    "import os\n",
    " \n",
    "import matplotlib\n",
    "from matplotlib.pyplot import imshow, show\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, Box(512, 288, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c272eda90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAE7pJREFUeJzt3X2MHdV5x/Hv412v7fXb2sYYYzuxDZRAqTDUUFJaSkxcKEGQP0ILjVDU0PJPmhISKUAjlSK1EkhVAFUVqgMkbkp4CYEGORRqGRCKEsAGnARsDMYQY2xsY2OD19m1d/fpHzOzd9ide+/cvS97x+f3kVY7d+68nDOzz55n5s49x9wdEQnLhPEugIi0ngJfJEAKfJEAKfBFAqTAFwmQAl8kQAp8kQDVFfhmdomZbTGzrWZ2U6MKJSLNZWN9gMfMOoA3gJXADmA9cLW7b2pc8USkGTrrWPdcYKu7bwMwsweBK4CygT+xa6pP7p5Vxy5FpJK+wx9y9EivVVuunsBfALyber0D+KNKK0zunsWyC66vY5ciUsnG5+7KtVw91/hZ/1VGXTeY2XVmtsHMNhw90lvH7kSkUeoJ/B3AotTrhcDOkQu5+yp3X+7uyyd2Ta1jdyLSKPUE/nrgFDNbYmZdwFXA440plog005iv8d19wMz+HngK6ADuc/fXGlYyEWmaem7u4e5PAE80qCwi0iJ6ck8kQAp8kQAp8EUCpMAXCZACXyRACnyRACnwRQKkwBcJkAJfJEAKfJEAKfBFAqTAFwmQAl8kQAp8kQAp8EUCpMAXCZACXyRAVQPfzO4zsz1m9mpq3mwzW2tmb8a/1Vm+SIHkafF/AFwyYt5NwDp3PwVYF78WkYKoGvju/hywf8TsK4DV8fRq4IsNLpeINNFYr/HnufsugPj38Y0rkog0W9Nv7mkkHZH2M9bA321m8wHi33vKLaiRdETaz1gD/3HgK/H0V4CfNqY4ItIKeT7OewD4JXCqme0ws2uB24CVZvYmsDJ+LSIFUXUkHXe/usxbFzW4LCLSInpyTyRACnyRACnwRQKkwBcJkAJfJEAKfJEAKfBFAqTAFwmQAl8kQAp8kQAp8EUCpMAXCZACXyRACnyRACnwRQKkwBcJkAJfJEB5ut5aZGbPmNlmM3vNzK6P52s0HZGCytPiDwDfcvfTgPOAr5nZ6Wg0HZHCytPn3i4gGTzjYzPbDCwgGk3nwnix1cCzwI1NKeU42f6F0vSnfjZ+5RBptJqu8c1sMXAW8AI5R9PRgBoi7adqi58ws2nAT4BvuPtHZpZrPXdfBawCmN6z0MdSyPGiVl6OVblafDObSBT097v7o/Hs3KPpiEh7yXNX34B7gc3u/t3UWxpNR6Sg8qT65wPXAL8xs43xvH8kGj3n4Xhkne3Alc0poog0Wp67+j8Hyl3QazQdkQLSk3siAVLgiwRIgS8SoNyf47eTyTfsHJ7uu+PEcSyJSDGpxRcJkAJfJECFTPWV3ovURy2+SIAU+CIBUuCLBEiBLxKgQt7cE6nXR3/30fD0jO/NGMeSjA+1+CIBUuCLBEipvgSpXdL7uTduG55++79PAWDae4NN369afJEAqcU/Buw9MzqNfccPDc9btHao3OLSRvbevnR4ehrNb+kTefrcm2xmL5rZr+KRdG6N5y8xsxfikXQeMrOu5hdXRBohT6rfD6xw9zOBZcAlZnYecDtwRzySzofAtc0rpog0Up4+9xw4FL+cGP84sAL463j+auCfgbsbX0SpZu6vBsa7CFIwefvV74h72N0DrAXeAg64e/IXt4NoWK2sdTWSjkibyRX47j7o7suAhcC5wGlZi5VZd5W7L3f35RO7po69pCLSMDV9nOfuB4gGxzwP6DGz5FJhIbCz3Hoi0l7y3NWfa2Y98fQU4PPAZuAZ4EvxYhpJR6RA8nyOPx9YbWYdRP8oHnb3NWa2CXjQzP4FeIVomC0RKYA8d/V/TTQ09sj524iu90WkYPTIrkiAFPgiAVLgiwRIgS8SIAW+SIAU+CIBUuCLBEiBLxIgBb5IgBT4IgFS4IsESIEvEiAFvkiAFPgiAVLgiwRIgS8SIAW+SIByB37cxfYrZrYmfq2RdEQKqpYW/3qiTjYTGklHpKDyDqixEPgCcE/82ohG0nkkXmQ18MVmFFBEGi9vi38n8G0gGYJ1DhpJR6Swqvaya2aXAXvc/SUzuzCZnbFo2ZF0gFUA03sWZi5TBFOe2gjAvscWD8/b/2E0MtDJ17wyPO+pnRtHrXvxicuGp/suU8fEMv7y9Kt/PnC5mV0KTAZmEGUAPWbWGbf6GklHpECqpvrufrO7L3T3xcBVwNPu/mU0ko5IYeVp8cu5kWN0JJ0JA9EVyTP3fS/j3ReHp5IUfusPS+ONLHli+fD025feA3wy/f/cV88BYKgz62pJpDVqCnx3f5Zo0EyNpCNSYHpyTyRA9aT6x6wkDf+z664bnjd5TZTi2zl/MDzvt7dOB+Dka34xPK/jlKXD0xf/belufqKL9YDu7sv4UosvEiC1+DklLfSU9w8Pz/v0Lb8YtdzQzO5R64i0G7X4IgFS4IsESKl+jX53QimVR6m8FJRafJEAKfBFAqTAFwmQAl8kQAp8kQAp8EUCpMAXCZACXyRACnyRAOV6cs/M3gE+BgaBAXdfbmazgYeAxcA7wF+6+4fNKaaINFItLf7n3H2Zuyd9S90ErIsH1FgXvxaRAqgn1b+CaCAN0IAaIoWS90s6DvyfmTnwn3Ff+fPcfReAu+8ys+ObVUiRRH9P5baqo780dEPn7wo7jEPT5Q388919Zxzca83s9bw7MLPrgOsAJk3pGUMRRaTRcgW+u++Mf+8xs8eIetfdbWbz49Z+PrCnzLoVR9I5PLdjePrIjKivu6PTK5dn6nulzXTvHcxTBRmjoYmlbsD74ta2d0HlrsHT5ydRy3lK/iaq7Wcs5rwWlWPC0bCzgarX+GY21cymJ9PAnwOvAo8TDaQBGlBDpFDytPjzgMeiAXLpBH7k7k+a2XrgYTO7FtgOXNm8YopII1UN/HjgjDMz5u8DLqplZ4NdxkefinbZP7uWNT8pnQL2LhhdhZlvDQ1Pd308NOr9VumdH6Wsh+fVl7J2747S0qm78qfLY9l3sp96y5uVomedp2R/jdhnXvt+v6Ph+67n/PTNKe17qEo0VtrPvjOilQfW56uLntwTCZACXyRALe1sc6izvhQ/r4Mnlf6fTdofTc/YPjBqucFJqUuGeR2j3k/LWn94f0tLh/HIjNzFzC1JRQ/PK+1n0v7S+406pq1Kt8drf83ad6XzMziltNzAFOqStZ+RPGe11OKLBOiY7147aQ33zq6vqvWu32ityJxk7Nr9/KjFFwmQAl8kQO2Vv7aZyR+UPuudumf08wAHlkY3BAcnt6xIdZmzqfLnzPtOr3yDs51VqltR6tXRF/3u2ZZdl97jo3a677j6b0yqxRcJUCFb/I4jpenu96NWecJA5S9dHFxa+X/cCS8eqfh+lgmDcYtf85qflH6KrKM/3nZGffpml+rQX+WLjtXqM9gVbavjSDqTaUzLOH376LLXe36SFn3iocpHO6kXpOvW+HqNx99bR5WvJNdCLb5IgBT4IgFqaao/sddHpThZTxrtPqcrc/2pO6P0qrMv/3epp3wwEP8uzXv7r6LfC3+WnQL6hLhQXtqPZexy2ntRKjnpQOmpvqz6HFowcXi698RogZnbav/yUP+s0vQJL5SO48GvfwTAwNPHjVpnuC4jTDg6ev9JmZJjBvnPz1jqk97PwZNK20zXLXF0Wr50Pate6b+5ak+2JXWbdKA0b/L+fHVL1yf5e9t1Xqle858fXa/0+bGhyn/XU/ZHlznTd5TKM7I+7/Xmiw21+CIBUuCLBGjc7uon6WnazH+PvuFSLm0cmFw+TztydelbK4fXl1LedIo/Urk7xNVSrkSS4h+aX0rlp+06Omq5JEUD6Oyr/L92Qpwt9l2zf9R7vn50Kp8257XRqWTeukApVU0uhQDmPje6bunzk/6koZLBVBdeWceoWgre0Zcv3c66JEvrPSGqz94LSmVY8lDp/aRu1er1/kWltD59jGpVy/npPDz6GCT1Tf4G012lVaIWXyRAeUfS6QHuAc4g6mr7q8AWjtGRdJJsJJ05ZN2YKYKszApKdStavaplV9VuprWbrOwqq16NlrfFvwt40t0/Q9QN12Y0ko5IYeXpZXcGcAFwL4C7H3H3A2gkHZHCypPqLwX2At83szOBl4DrqXMkneRGXjX9M0ffrBjqLM3rOuSjtjeTyilecjPnwEnZN2V6kpuMVbZTbv16DMS9AvkTc0a/OTd7nWTZAyeV5iVf+Gi2zsPR8c86TwCTDo69//qs49vfk75JWPv603fke8C62mf3szaUtj3QXWG5N0rbyf57qS+tH7lNz5nD51msEzgbuNvdzwJ6qSGtN7PrzGyDmW04eqQ372oi0kTmXvk/spmdADzv7ovj139KFPgnAxemRtJ51t1PrbSt7uMX+e9deUPZ9+tpHWppcY5MG71skjk0Y98Dqb79kq/wlqtrsq1ajkWldcqVbWQZyy2bbiGPTG/Mh0B565guT97jkT7Wnf3lsxFLNfzpc5/3WE48VJo/YbB82bLKk5b+W/SMhxPzHqNkuVf/9056971b9TO9qmfS3d8H3jWzJKgvAjahkXRECivvAzxfB+43sy5gG/A3RP80NJKOSAHlHTRzI7A8462aRtKxwXwpW7UUb6ij9P7RaeWXKydvWl+tHEmaVi5tTKRTvM7+yvuslGKWS8urHaNKy1V7Pyu9T+87uYmYlcaml01vO++5qnaZlk6Nk2XLlWPkclmXe9XKVu695FinU/6k3tVusmbVoRZjvTzWk3siAWrLHniq/RdL/2eddHD8ylHPDcF6y5H3GDXj+NTSytRzwzbLWI55Vhkaee6ybu7Vk9W0glp8kQAp8EUCpMAXCZACXyRACnyRACnwRQKkwBcJUEs/x+/Y38uMHz3fyl2KBKXD830DVi2+SIAU+CIBammq3/+pbt74zrmfmLdi2aaK6zy98fRc227Udqptq922U8u2jtXtVNtWu22nlm3Vup3+f/1lru2qxRcJUEtb/OndfRX/g728eyEAB3aV6Y8vHj1gxZmbK+4n2U7ZbbXpdqptq1HbSW+rqdtJbavdjvUxe87yjaehFl8kRAp8kQDl6WzzVKIRcxJLgX8C/osaR9KZetwiP/2y8p1tikh9Nq25g94PGtPZ5hZ3X+buy4A/BA4Dj6GRdEQKq9ZU/yLgLXf/LRpJR6Swar2rfxXwQDxd80g6M+YdYuU3f17jLkUkrx3rD1VfiBpa/Lhr7cuBH9dSkPRIOoc/rNLFrIi0RC2p/l8AL7v77vj17ngEHeLfe7JWcvdV7r7c3Zd3z5pUX2lFpCFqCfyrKaX5oJF0RAorV+CbWTewEng0Nfs2YKWZvRm/d1vjiycizZB3JJ3DwJwR8/ZR40g6ItIe9OSeSIAU+CIBUuCLBEiBLxIgBb5IgBT4IgFS4IsESIEvEiAFvkiAFPgiAVLgiwRIgS8SIAW+SIAU+CIBUuCLBEiBLxIgBb5IgPJ2vXWDmb1mZq+a2QNmNtnMlpjZC2b2ppk9FPfCKyIFUDXwzWwB8A/Acnc/A+gg6l//duCOeCSdD4Frm1lQEWmcvKl+JzDFzDqBbmAXsAJ4JH5fI+mIFEiesfPeA/4N2E4U8AeBl4AD7j4QL7YDWNCsQopIY+VJ9WcRjZO3BDgRmEo0uMZImcPuaiQdkfaTJ9X/PPC2u+9196NEfev/MdATp/4AC4GdWStrJB2R9pMn8LcD55lZt5kZUV/6m4BngC/Fy2gkHZECyXON/wLRTbyXgd/E66wCbgS+aWZbiQbbuLeJ5RSRBso7ks4twC0jZm8Dzm14iUSk6fTknkiAFPgiAVLgiwRIgS8SIHPPfO6mOTsz2wv0Ah+0bKfNdxyqT7s6luoC+erzaXefW21DLQ18ADPb4O7LW7rTJlJ92texVBdobH2U6osESIEvEqDxCPxV47DPZlJ92texVBdoYH1afo0vIuNPqb5IgFoa+GZ2iZltMbOtZnZTK/ddLzNbZGbPmNnmuP/B6+P5s81sbdz34Nq4/4LCMLMOM3vFzNbErwvbl6KZ9ZjZI2b2enyePlvk89PMvi5bFvhm1gH8B1EnHqcDV5vZ6a3afwMMAN9y99OA84CvxeW/CVgX9z24Ln5dJNcDm1Ovi9yX4l3Ak+7+GeBMonoV8vw0va9Ld2/JD/BZ4KnU65uBm1u1/ybU56fASmALMD+eNx/YMt5lq6EOC4mCYQWwBjCiB0Q6s85ZO/8AM4C3ie9bpeYX8vwQdWX3LjCb6Fu0a4CLG3V+WpnqJxVJFLafPjNbDJwFvADMc/ddAPHv48evZDW7E/g2MBS/nkNx+1JcCuwFvh9futxjZlMp6PnxJvd12crAt4x5hftIwcymAT8BvuHuH413ecbKzC4D9rj7S+nZGYsW5Rx1AmcDd7v7WUSPhhcirc9Sb1+X1bQy8HcAi1Kvy/bT167MbCJR0N/v7o/Gs3eb2fz4/fnAnvEqX43OBy43s3eAB4nS/TvJ2ZdiG9oB7PCoxyiIeo06m+Ken7r6uqymlYG/HjglvivZRXSj4vEW7r8ucX+D9wKb3f27qbceJ+pzEArU96C73+zuC919MdG5eNrdv0xB+1J09/eBd83s1HhW0jdkIc8Pze7rssU3LC4F3gDeAr4z3jdQaiz7nxClVb8GNsY/lxJdF68D3ox/zx7vso6hbhcCa+LppcCLwFbgx8Ck8S5fDfVYBmyIz9H/ALOKfH6AW4HXgVeBHwKTGnV+9OSeSID05J5IgBT4IgFS4IsESIEvEiAFvkiAFPgiAVLgiwRIgS8SoP8HowSq3rpBDRMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.init()\n",
    "env.step(1)\n",
    "imshow(env.get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.dqn = DQN().to(device)\n",
    "        self.target = DQN().to(device)\n",
    "        self.target.load_state_dict(self.dqn.state_dict())\n",
    "        self.target.eval()\n",
    "        self.target_update_interval = 1000\n",
    "#         self.optimizer = optim.RMSprop(self.dqn.parameters())\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.0001)\n",
    "        \n",
    "        #play params\n",
    "        self.play_interval = 1000\n",
    "        self.play_repeat = 1\n",
    "        self.best_steps_done = 0\n",
    "        self.best_score = -1000\n",
    "        \n",
    "        self.seed = 111\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_decay = 100000\n",
    "        self.epsilon_stop = 0.01\n",
    "        self.batch_size = 32\n",
    "#         self.max_step = 1000\n",
    "        self.global_steps = 0\n",
    "        \n",
    "        self.memory = Memory(50000, batch_size=self.batch_size)\n",
    "\n",
    "        self.frame_skipping = 4\n",
    "        self.state_buffer_size = 4\n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        \n",
    "    def save_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        dirpath = os.path.dirname(filename)\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.mkdir(dirpath)\n",
    "\n",
    "        checkpoint = {\n",
    "            'dqn': self.dqn.state_dict(),\n",
    "            'target': self.target.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'global_steps': self.global_steps,\n",
    "            'best_score': self.best_score,\n",
    "            'best_steps_done': self.best_steps_done\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    def load_checkpoint(self, filename='checkpoints/checkpoint.pth'):\n",
    "        checkpoint = torch.load(filename, map_location=device.type)\n",
    "        self.dqn.load_state_dict(checkpoint['dqn'])\n",
    "        self.target.load_state_dict(checkpoint['target'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.global_steps = checkpoint['global_steps']\n",
    "        self.best_score = checkpoint['best_score']\n",
    "        self.best_steps_done = checkpoint['best_steps_done']\n",
    "        \n",
    "        \n",
    "    def play(self, human=True):\n",
    "        self.env.reset()\n",
    "        state = self.get_initial_state()\n",
    "        steps_done = 0\n",
    "        total_score = 0\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            \n",
    "            self.dqn.eval()\n",
    "            action = self.dqn(state).cpu().max(1)[1]\n",
    "\n",
    "            for _ in range(self.frame_skipping):\n",
    "                if human:\n",
    "                    screen = self.env.game.render(mode='human')\n",
    "                _, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                next_state = self.env.get_screen()\n",
    "                self.state_buffer.append(next_state)\n",
    "                state = np.array(self.state_buffer)\n",
    "\n",
    "                total_score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            steps_done += 1\n",
    "            \n",
    "            if done:\n",
    "                self.dqn.train()\n",
    "                break\n",
    "        self.env.game.close()\n",
    "        return total_score, steps_done\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_screen()\n",
    "        \n",
    "        self.state_buffer = deque(maxlen=self.state_buffer_size)\n",
    "        [self.state_buffer.append(state) for _ in range(self.state_buffer_size)]\n",
    "        return np.array(self.state_buffer)\n",
    "    \n",
    "    def optimize(self):\n",
    "        transitions = self.memory.sample()\n",
    "\n",
    "        states = torch.cat(transitions.state).to(device)\n",
    "        actions = torch.cat(transitions.action).to(device)\n",
    "        rewards = torch.cat(transitions.reward).to(device)\n",
    "        \n",
    "        non_final_mask = torch.tensor(list(map(lambda s: s is not None, transitions.next_state)), \n",
    "                                      device=device, dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([s for s in transitions.next_state\n",
    "                                            if s is not None])\n",
    "        \n",
    "        states = states.view([self.batch_size, self.state_buffer_size, self.env.width, self.env.height])\n",
    "        q_values = self.dqn(states).gather(1, actions)\n",
    "        \n",
    "        target_values = torch.zeros(self.batch_size, device=device)\n",
    "        non_final_next_states = non_final_next_states.view([-1, self.state_buffer_size, self.env.width, self.env.height]).to(device)\n",
    "        target_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        rewards.data.clamp_(-1, 1)\n",
    "        \n",
    "        expected_state_action_values = (target_values * self.gamma) + rewards\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.dqn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        reward_score = int(torch.sum(rewards).cpu().detach().numpy())\n",
    "        \n",
    "        return loss.cpu().detach().numpy(), reward_score\n",
    "\n",
    "\n",
    "    def action(self, state):\n",
    "        eps = self.epsilon_stop + (self.epsilon_start - self.epsilon_stop) * np.exp(-self.global_steps/self.epsilon_decay)\n",
    "        if np.random.uniform() <= eps:\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            with torch.no_grad():\n",
    "                return self.dqn(state).max(1)[1].view(-1,1).cpu(), eps\n",
    "        else:\n",
    "            sample_action = self.env.game.action_space.sample()\n",
    "            action = torch.LongTensor([[sample_action]])\n",
    "            return action, eps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        while True:\n",
    "            self.env.reset()\n",
    "            state = self.get_initial_state()\n",
    "            \n",
    "            losses = []\n",
    "            rewards = []\n",
    "            steps_done = 0\n",
    "            eps = self.epsilon_start\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                action, e = self.action(state)\n",
    "                eps = e\n",
    "                \n",
    "                for _ in range(self.frame_skipping):\n",
    "                    _, reward, done, _ = self.env.step(action.item())\n",
    "                    next_state = self.env.get_screen()\n",
    "                    self.state_buffer.append(next_state)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "            \n",
    "            \n",
    "                next_state = np.array(self.state_buffer)\n",
    "                \n",
    "                if done:\n",
    "                    self.memory.add(state, action, reward, None)\n",
    "                else:\n",
    "                    self.memory.add(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                if self.memory.is_ready():\n",
    "                    l, r = self.optimize()\n",
    "                    losses.append(l)\n",
    "                    rewards.append(r)\n",
    "\n",
    "                steps_done += 1\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                # update target network\n",
    "                if self.global_steps % self.target_update_interval == 0:\n",
    "                    print('update target network!')\n",
    "                    self.target.load_state_dict(self.dqn.state_dict())\n",
    "              \n",
    "                # Play\n",
    "                if self.global_steps % self.play_interval == 0:\n",
    "                    scores = []\n",
    "                    total_steps_done = []\n",
    "                    for _ in range(self.play_repeat):\n",
    "                        score, steps_done = self.play(human=True)\n",
    "                        scores.append(score)\n",
    "                        total_steps_done.append(steps_done)\n",
    "                        \n",
    "                    real_score = int(np.mean(scores))\n",
    "                    real_steps_done = int(np.mean(total_steps_done))\n",
    "\n",
    "                    if self.best_steps_done <= real_steps_done:\n",
    "                        self.best_score = real_score\n",
    "                        self.best_steps_done = real_steps_done\n",
    "\n",
    "                        self.save_checkpoint(\n",
    "                            filename=f'dqn_checkpoints/chkpoint_{self.best_steps_done}_{self.best_score}.pth')\n",
    "\n",
    "            if not np.isnan(np.mean(losses)):\n",
    "                print('global_steps:%d, episode_steps:%d, loss:%.6f, rewards:%d, eps:%.6f'%(self.global_steps, steps_done, np.mean(losses), np.mean(rewards), eps))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "# agent = Agent() \n",
    "# agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_checkpoint('dqn_checkpoints/exp_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.0, 197)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play\n",
    "agent = Agent()\n",
    "agent.load_checkpoint('dqn_checkpoints/chkpoint_771_76.pth')\n",
    "agent.play(human=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://github.com/AndersonJo/dqn-pytorch/blob/master/dqn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
